# Comments are provided throughout this file to help you get started.
# If you need more help, visit the Docker compose reference guide at
# https://docs.docker.com/go/compose-spec-reference/

# Here the instructions define your application as a service called "server".
# This service is built from the Dockerfile in the current directory.
# You can add other services your application may depend on here, such as a
# database or a cache. For examples, see the Awesome Compose repository:
# https://github.com/docker/awesome-compose

version: '3'

x-spark-common: &spark-common
  image: bitnami/spark:latest
  volumes:
      - ./jobs:/opt/bitnami/spark/jobs
  command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  depends_on:
    - spark-master
  environment:
    SPARK_MODE: Worker
    SAPRK_WORKER_CORES: 2
    SPARK_WORKER_MEMORY: 1g
    SPARK_MASTER_URL: spark://spark-master:7077
  networks:
    - datamasterylab



services:
  server:
    build:
      context: .
    ports:
      - 8000:8000

  # zookeeper:
  #   image: confluentinc/cp-zookeeper:6.0.2-1-ubi8 
  #   hostname: zookeeper
  #   container_name: material
  #   ports:
  #     - "2181:2181"
  #   environment:
  #       ZOOKEEPER_CLIENT_PORT: 2181
  #       ZOOKEEPER_TICK_TIME: 2000
  #   healthcheck:
  #     test: ['CMD' , 'bash' , '-c' , "echo 'ruok' | nc localhost 2181"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     # start_period: 30s
  #   networks:
  #     - datamasterylab
  # broker:
  #   image: confluetinc/cp-server:7.4.0
  #   hostname: broker
  #   depends_on:
  #       zookeeper:
  #         condition: service_healthy
  #   ports:
  #     - "9092:9092"
  #     - "9101:9101"
  #   environment:
  #       KAFKA_BROKER_ID : 1
  #       KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
  #       KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT, PLAINTEXT_HOST:PLAINTEXT
  #       KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
  #       KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.confluentMetricsReporter
  #       KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  #       KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
  #       KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
  #       KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
  #       KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
  #       KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
  #       KAFKA_JMX_PORT: 9101
  #       KAFKA)JMX_HOSTNAME: localhost
  #       KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081
  #       CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092
  #       CONFLUENT_mterics_reporter_topic_replicas: 1
  #       CONFLUENT_METRICS_ENABLE: 'false'
  #       CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
  #   healthcheck:
  #     test: ['CMD' , 'bash' , '-c' , "nc -z localhost 9092"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5

    networks:
      datamasterylab:

  spark-master:
    image: bitnami/spark:latest
    volumes:
      - ./jobs:/opt/bitnami/spark/jobs
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"
    networks:
      - datamasterylab

  spark-worker-1:
    <<: *spark-common
  spark-worker-2:
    <<: *spark-common
  spark-worker-3:
    <<: *spark-common
  spark-worker-4:
    <<: *spark-common

    


         
networks:
  datamasterylab:


# The commented out section below is an example of how to define a PostgreSQL
# database that your application can use. `depends_on` tells Docker Compose to
# start the database before your application. The `db-data` volume persists the
# database data between container restarts. The `db-password` secret is used
# to set the database password. You must create `db/password.txt` and add
# a password of your choosing to it before running `docker compose up`.
#     depends_on:
#       db:
#         condition: service_healthy
#   db:
#     image: postgres
#     restart: always
#     user: postgres
#     secrets:
#       - db-password
#     volumes:
#       - db-data:/var/lib/postgresql/data
#     environment:
#       - POSTGRES_DB=example
#       - POSTGRES_PASSWORD_FILE=/run/secrets/db-password
#     expose:
#       - 5432
#     healthcheck:
#       test: [ "CMD", "pg_isready" ]
#       interval: 10s
#       timeout: 5s
#       retries: 5
# volumes:
#   db-data:
# secrets:
#   db-password:
#     file: db/password.txt

  